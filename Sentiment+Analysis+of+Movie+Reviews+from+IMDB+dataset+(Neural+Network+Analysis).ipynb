{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Movie Reviews from IMDB dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurring Neural Network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to use IMDB dataset that consists of user generated movie reviews and classification of whether the user liked the movie or not based on its associated rating.\n",
    "\n",
    "**We are using RNN to do some sentiment analysis on full-text movie reviews!**\n",
    "\n",
    "**Steps to follow:**\n",
    "\n",
    "To train an artificial network how to \"read\" movie reviews and guess whether the author liked the movie or not from them  \n",
    "\n",
    "We need a recurrent neural network to keep a \"memory\" of the words that have come before as it \"reads\" sentences over the time because understanding written language demands keeping track of all the words in a sentence \n",
    "\n",
    "Words that have been used early on in a sentence can affect the meaning of the sentence so to avoid such issues we will use (Long Short-Term Memory) cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import  our training and testing data. we specify that we only care about the 20000 most popular words in the dataset in order to keep things somewhat manageable. The data includes 5000 training reviews and 25000 testing reviews for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=20000,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get an idea of how and what this data looks like. the training feature should represent a written movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesnot look like a movie review but this dataset has spared us a lot of trouble. they have already converted words to integer based indices. the actual letters that made up a word dont really matter as far as our model is concerned. what matters is the word themselves and our model needs numbers to work with, not letters.\n",
    "hence each number represent some specific words in the training features. its a bummer that we cant just read the reviews in english as a gut check to see if sentiment analysis is really working though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are just 0 or 1, which indicates whether the reviewer said they liked the movie or not. bunch of movie reviews have been converted into vectors of words represented by integers, and a binary sentiment classification to learn from. RNN can blow up quickly, so again to keep things manageable on our little PC lets limit the reviews to first 100 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up our neural network model!we will start with an embedding layer- \n",
    "\n",
    "This is just a step that converts the input data into dense vectors of fixed size thats better suited for a neural network you generally see this in conjunction with index based text data like we have here. the 20000 indicates the vocabulary size(remember we said we only wanted the top 20000 words)and 128 is the output of 128 units.\n",
    "\n",
    "Next we just have to set up a LSTM layer for the RNN itself. its that easy we specify 128 to match the output size of the embedding layer and dropout terms to avoid overfitting, which rnn's  are particularly prone to. \n",
    "\n",
    "Finally we just need to boil down it down to a single neuron with a sigmoid activation function to choose our binary sentiment classificatiom of 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = Sequential()\n",
    "review.add(Embedding(20000,128))\n",
    "review.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "review.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a binary classification problem, we'll use the binary_crossentropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to train our model review. RNN is resource heavy. Keeping the batch size relatively small is the best way to run on pc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      " - 261s - loss: 0.4662 - acc: 0.7798 - val_loss: 0.3977 - val_acc: 0.8188\n",
      "Epoch 2/15\n",
      " - 272s - loss: 0.3125 - acc: 0.8731 - val_loss: 0.3657 - val_acc: 0.8471\n",
      "Epoch 3/15\n",
      " - 255s - loss: 0.2232 - acc: 0.9121 - val_loss: 0.3822 - val_acc: 0.8357\n",
      "Epoch 4/15\n",
      " - 245s - loss: 0.1617 - acc: 0.9392 - val_loss: 0.4182 - val_acc: 0.8395\n",
      "Epoch 5/15\n",
      " - 246s - loss: 0.1147 - acc: 0.9577 - val_loss: 0.5426 - val_acc: 0.8331\n",
      "Epoch 6/15\n",
      " - 245s - loss: 0.0893 - acc: 0.9688 - val_loss: 0.5729 - val_acc: 0.8314\n",
      "Epoch 7/15\n",
      " - 244s - loss: 0.0605 - acc: 0.9796 - val_loss: 0.6191 - val_acc: 0.8299\n",
      "Epoch 8/15\n",
      " - 255s - loss: 0.0448 - acc: 0.9847 - val_loss: 0.7623 - val_acc: 0.8294\n",
      "Epoch 9/15\n",
      " - 273s - loss: 0.0366 - acc: 0.9874 - val_loss: 0.8108 - val_acc: 0.8221\n",
      "Epoch 10/15\n",
      " - 259s - loss: 0.0332 - acc: 0.9896 - val_loss: 0.8003 - val_acc: 0.8286\n",
      "Epoch 11/15\n",
      " - 245s - loss: 0.0230 - acc: 0.9928 - val_loss: 0.8536 - val_acc: 0.8284\n",
      "Epoch 12/15\n",
      " - 246s - loss: 0.0206 - acc: 0.9935 - val_loss: 0.9766 - val_acc: 0.8266\n",
      "Epoch 13/15\n",
      " - 245s - loss: 0.0159 - acc: 0.9948 - val_loss: 1.0249 - val_acc: 0.8149\n",
      "Epoch 14/15\n",
      " - 246s - loss: 0.0114 - acc: 0.9967 - val_loss: 1.1065 - val_acc: 0.8209\n",
      "Epoch 15/15\n",
      " - 251s - loss: 0.0088 - acc: 0.9968 - val_loss: 1.1509 - val_acc: 0.8226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b4a504def0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.fit(x_train, y_train, batch_size = 32, epochs = 15, verbose = 2, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, Let's evaluate our model's accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 1.1509044100207277\n",
      "Test accuracy: 0.82256\n"
     ]
    }
   ],
   "source": [
    "score, acc = review.evaluate(x_test, y_test, batch_size=32, verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Initially when i started working on it, i didnot set maxlen in sequencepad and got accuracy close to 83%, i was worried for the data to blow up without any limits so i had set it to 120 then eventually 100 which seemed optimum to me. Adam is usually a good optimizer so i decided to go with it for tuning the model. Choosing number of epochs and running code took me forever. However i changed the no. of epochs, observed the results and compared them. i have changed it from 15 to 20, 20 to 50, and lastly 50 to 15. The results due to the change in no. of epochs was'nt producing a major change in the accuracy. Hence i decided to go with 15 epochs and managed to score an accuracy of 82.256%\n",
    "\n",
    "**Considering we limited ourselves to 100 words of each review, accuracy is not bad.\n",
    "with such amount of minimal  code we have a neural network that can read reviews and deduce whether the author liked the movie or not based on that text. And it takes the context of each word and its position in the review into account. It's pretty incredible what we can do with keras**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
